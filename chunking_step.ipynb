{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import tqdm\n",
    "import logging\n",
    "from nltk import untag\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "import string\n",
    "import json, urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/Downloads/migrants_data/' # путь до папки с текстами по годам (которые 2015.csv и т.д.)\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['year'] = pd.to_numeric(filename.replace('.csv', '')[-4:])\n",
    "    li.append(df)\n",
    "\n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['description', 'year']]\n",
    "data.columns = ['text', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "#### Приводим слова к начальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_text = []\n",
    "for i in tqdm.tqdm(data['text']):\n",
    "    try:\n",
    "        lemmas = m.lemmatize(i)\n",
    "        lem = ''.join(lemmas)\n",
    "        lem_text.append(lem)\n",
    "    except:\n",
    "        lem_text.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lem_text'] = lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на всякий случай сохраняем данные с лемматизированными текстами в csv\n",
    "data.to_csv('~/Downloads/migrants_data/lem_texts_migration.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(['\\n', '\\s+'])\n",
    "data['lem_text'] = data['lem_text'].str.replace(pattern, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lem_text'] = data['lem_text']\\\n",
    "    .apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "#### Устанавливаем правила, по которым выделяются словосочетания из ранее предобработанных текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = ('''\n",
    "    AS: {<A=m><S>}\n",
    "    SS: {<S><S>}\n",
    "    S1: {<S>}\n",
    "    ''')\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_text = list(data['lem_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "i = 0\n",
    "for text in tqdm.tqdm(lem_text):\n",
    "    i = i + 1\n",
    "    num_text = \"text {}\".format(i)\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text), lang='rus')\n",
    "    tree = chunkParser.parse(tagged)\n",
    "    as_tag = []\n",
    "    ss_tag = []\n",
    "    s_tag = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'AS': as_tag.append(\" \".join(untag(subtree)).lower())\n",
    "        if subtree.label() == 'SS': ss_tag.append(\" \".join(untag(subtree)).lower())\n",
    "        if subtree.label() == 'S1': s_tag.append(\" \".join(untag(subtree)).lower())\n",
    "    as_tag_df = pd.DataFrame({'term': as_tag})\n",
    "    as_tag_df['tag'] = 'as_tag'\n",
    "    \n",
    "    ss_tag_df = pd.DataFrame({'term': ss_tag})\n",
    "    ss_tag_df['tag'] = 'ss_tag'\n",
    "    \n",
    "    s_tag_df = pd.DataFrame({'term': s_tag})\n",
    "    s_tag_df['tag'] = 's_tag'\n",
    "\n",
    "    df = as_tag_df.append(ss_tag_df)\n",
    "    df = df.append(s_tag_df)\n",
    "    df['num_text'] = num_text\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_tag = list(final_df[final_df['tag'] == 'ss_tag'].groupby('term').count().sort_values('tag', ascending=False).head(1000)\\\n",
    "    .reset_index()['term'])\n",
    "\n",
    "as_tag = list(final_df[final_df['tag'] == 'as_tag'].groupby('term').count().sort_values('tag', ascending=False).head(1000)\\\n",
    "        .reset_index()['term'])\n",
    "\n",
    "s_tag = list(final_df[final_df['tag'] == 's_tag'].groupby('term').count().sort_values('tag', ascending=False).head(1000)\\\n",
    "    .reset_index()['term'])\n",
    "\n",
    "import itertools\n",
    "tags = itertools.chain(ss_tag, as_tag, s_tag)\n",
    "\n",
    "terms = []\n",
    "for i in tags:\n",
    "    terms.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = []\n",
    "for i in tqdm.tqdm(terms):\n",
    "    try:\n",
    "        temp = final_df.query(\"term == '{}'\".format(str(i)))\n",
    "        fin.append(temp)\n",
    "    except:\n",
    "        Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df_with_top_words = pd.concat(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_text'] = data.index + 1\n",
    "data['num_text'] = 'text ' + data['num_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = data['num_text'].map(lambda x: x.lstrip('text '))\n",
    "fin_df_with_top_words['id'] = fin_df_with_top_words['num_text'].map(lambda x: x.lstrip('text '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_term = data[['id', 'year']].merge(fin_df_with_top_words, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_term = date_term.merge(term_count, on=['year','term'])\n",
    "date_term = date_term[['term', 'year', 'id', 'Time']]\n",
    "date_term.columns = ['term', 'cat', 'id', 'count']\n",
    "date_term.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файл со списком словосочетаний для каждого года\n",
    "#### Записываем в файлы по годам словосочетания из текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗Константа **частота встречаемости словосочетания по всему корпусу текстов**\n",
    "\n",
    "    Чем больше хотим получить слов в анализ, тем меньше будет это значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_term = date_term[date_term['count'] > term_frequency]\n",
    "for y in date_term['cat'].unique():\n",
    "    date_term[date_term['cat'] == y]\\\n",
    "    [['term', 'cat', 'id', 'count']]\\\n",
    "    .drop_duplicates()\\\n",
    "    .to_csv(\"~/Downloads/migrants_data/migration_terms_{}.csv\".format(int(y)), index=False) \n",
    "    # сейчас здесь та же папка, где лежали входные тексты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
